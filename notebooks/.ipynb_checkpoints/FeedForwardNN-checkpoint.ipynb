{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e0074e6-c984-4247-840d-11e7956ff488",
   "metadata": {},
   "source": [
    "### Training and Modeling with Feed-Forward Neural Network (FNN) for RUL Estimation\n",
    "Implemented a five-layer FNN for RUL prediction, trained over 100 epochs with real-time monitoring of training and validation loss. This approach emphasizes overfitting prevention and hyperparameter tuning, ensuring effective learning and generalization, crucial in predictive maintenance within aerospace engineering \\\n",
    "\\\n",
    "Implemented FNN model as per specifications in the paper:\n",
    "Fusing physics-based and deep learning models for prognosticsn'.\n",
    "Availablehttps://www.sciencedirect.com/science/article/pii/S0951832021004725?via%3Dihub2\"\"\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb9eba9-9ad4-4126-8e65-f949176c32d8",
   "metadata": {},
   "source": [
    "### Loading the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a259a31e-84a1-48da-98b9-6891b748bf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Specify the full path to the HDF5 file including the folder\n",
    "hdf5_filename = '/mnt/e/PdM/DataX/NASA/Temp_Data/processed_data.h5'\n",
    "\n",
    "# Open the HDF5 file for reading\n",
    "hdf5_file = h5py.File(hdf5_filename, 'r')\n",
    "\n",
    "# Load the datasets into variables\n",
    "X = hdf5_file['X_train'][:]\n",
    "X_test = hdf5_file['X_test'][:]\n",
    "y = hdf5_file['y'][:]\n",
    "y_test = hdf5_file['y_test'][:]\n",
    "\n",
    "# Close the HDF5 file\n",
    "hdf5_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7de22d49-a230-4212-b7b7-c333dff98e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_20 (Dense)            (None, 200)               3800      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 200)               40200     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 200)               40200     \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 50)                10050     \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 94,301\n",
      "Trainable params: 94,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import GlorotNormal\n",
    "\n",
    "# Assuming the number of features (n) in the input space x is already known\n",
    "n_features = X.shape[1]  # Replace with the actual number of features if not using X from earlier steps\n",
    "\n",
    "# Xavier (Glorot) initializer\n",
    "initializer = GlorotNormal()\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Dense(200, input_dim=n_features, activation='relu', kernel_initializer=initializer),  # Input layer with n nodes and first hidden layer with 200 neurons\n",
    "    Dropout(0.3),\n",
    "    Dense(200, activation='relu', kernel_initializer=initializer),  # Second hidden layer with 200 neurons\n",
    "    Dropout(0.3),\n",
    "    Dense(200, activation='relu', kernel_initializer=initializer),  # Third hidden layer with 200 neurons\n",
    "    Dense(50, activation='relu', kernel_initializer=initializer),   # Fourth hidden layer with 50 neurons\n",
    "    Dense(1, activation='linear', kernel_initializer=initializer)   # Output layer with a single neuron (linear activation for regression)\n",
    "])\n",
    "\n",
    "\n",
    "# Compile the model with AMSgrad\n",
    "model.compile(optimizer=Adam(learning_rate=0.001, amsgrad=True), loss='mean_squared_error', )\n",
    "              # metrics=['mean_squared_error'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e7d0be-0d73-4cf1-9bf4-bd7c90e16aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "2251/2251 [==============================] - 84s 37ms/step - loss: 28.1272 - val_loss: 12.1502\n",
      "Epoch 2/60\n",
      "2251/2251 [==============================] - 82s 36ms/step - loss: 27.4213 - val_loss: 13.9917\n",
      "Epoch 3/60\n",
      "2251/2251 [==============================] - 83s 37ms/step - loss: 26.9351 - val_loss: 17.6269\n",
      "Epoch 4/60\n",
      "1382/2251 [=================>............] - ETA: 28s - loss: 27.2008"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define training parameters\n",
    "epochs = 60  # Number of epochs to train\n",
    "batch_size = 512  # Batch size for training\n",
    "\n",
    "# Calculate the split index for 70% training and 20% validation\n",
    "split_index = int(len(X) * 0.9)\n",
    "\n",
    "# split_index = 4069786\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val = X[:split_index], X[split_index:]\n",
    "y_train, y_val = y[:split_index], y[split_index:]\n",
    "\n",
    "\n",
    "# Train the model with the specified training and validation sets\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb20d807-885a-4253-966a-027a0d5de774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim(0, 100)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128dcbef-5aaf-431d-a57f-87638162cfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('fnn.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
