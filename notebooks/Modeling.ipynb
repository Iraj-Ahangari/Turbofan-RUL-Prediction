{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84a140e5-dabb-4f04-8a23-6c857871e515",
   "metadata": {},
   "source": [
    "### Modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afdddb7-7037-43aa-ba55-1396da969a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import GlorotNormal\n",
    "\n",
    "# Assuming the number of features (n) in the input space x is already known\n",
    "n_features = X.shape[1]  # Replace with the actual number of features if not using X from earlier steps\n",
    "\n",
    "# Xavier (Glorot) initializer\n",
    "initializer = GlorotNormal()\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Dense(200, input_dim=n_features, activation='relu', kernel_initializer=initializer),  # Input layer with n nodes and first hidden layer with 200 neurons\n",
    "    Dense(200, activation='relu', kernel_initializer=initializer),  # Second hidden layer with 200 neurons\n",
    "    Dense(200, activation='relu', kernel_initializer=initializer),  # Third hidden layer with 200 neurons\n",
    "    Dense(50, activation='relu', kernel_initializer=initializer),   # Fourth hidden layer with 50 neurons\n",
    "    Dense(1, activation='linear', kernel_initializer=initializer)   # Output layer with a single neuron (linear activation for regression)\n",
    "])\n",
    "\n",
    "def nasa_score_tf(y_true, y_pred):\n",
    "    delta = y_pred - y_true\n",
    "    factor_under = -1 / 13\n",
    "    factor_over = 1 / 10\n",
    "    \n",
    "    # Calculate score for under-estimation and over-estimation\n",
    "    score_under = tf.exp(tf.abs(delta * factor_under))\n",
    "    score_over = tf.exp(tf.abs(delta * factor_over))\n",
    "\n",
    "    # Apply conditions\n",
    "    score = tf.where(delta < 0, score_under, score_over)\n",
    "\n",
    "    # Sum up the scores\n",
    "    total_score = tf.reduce_sum(score)\n",
    "    return total_score\n",
    "\n",
    "# Compile the model with AMSgrad\n",
    "model.compile(optimizer=Adam(learning_rate=0.001, amsgrad=True), loss=nasa_score_tf)\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115c69b7-8f03-4258-85b5-2aa0f4f04e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "epochs = 60  # Number of epochs to train\n",
    "batch_size = 1024  # Batch size for training\n",
    "\n",
    "# Calculate the split index for 80% training and 20% validation\n",
    "split_index = int(len(X_normalized) * 0.8)\n",
    "\n",
    "# split_index = 4069786\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val = X_normalized[:split_index], X_normalized[split_index:]\n",
    "y_train, y_val = y[:split_index], y[split_index:]\n",
    "\n",
    "\n",
    "# Train the model with the specified training and validation sets\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec8612f-241e-4923-9a8f-aa7b0df7a599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e71438-92a8-4e71-b1ce-18b821efe77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming scaler is the MinMaxScaler instance used for training data\n",
    "# and model is the trained neural network model\n",
    "\n",
    "# Normalize the test data\n",
    "test_df_normalized = scaler.transform(test_df_sub.drop(columns=['RUL']))\n",
    "\n",
    "# Predict RUL\n",
    "predicted_RUL = model.predict(test_df_normalized)\n",
    "\n",
    "# Plotting Predicted vs Actual RUL\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(test_df_sub['RUL'], label='Actual RUL')\n",
    "plt.plot(predicted_RUL, label='Predicted RUL', alpha=0.7)\n",
    "plt.title('Comparison of Predicted and Actual RUL')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('RUL')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
