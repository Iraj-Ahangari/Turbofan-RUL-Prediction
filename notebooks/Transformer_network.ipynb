{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134a1c70-1b24-4852-97b8-0a121199df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Specify the full path to the HDF5 file including the folder\n",
    "hdf5_filename = '/mnt/e/PdM/DataX/NASA/Temp_Data/processed_data.h5'\n",
    "\n",
    "# Open the HDF5 file for reading\n",
    "hdf5_file = h5py.File(hdf5_filename, 'r')\n",
    "\n",
    "# Load the datasets into variables\n",
    "X = hdf5_file['X_train'][:]\n",
    "X_test = hdf5_file['X_test'][:]\n",
    "y = hdf5_file['y'][:]\n",
    "y_test = hdf5_file['y_test'][:]\n",
    "\n",
    "# Close the HDF5 file\n",
    "hdf5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c83dbaa-c31c-4402-ac88-cb94cda68b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the split index for 70% training and 20% validation\n",
    "split_index = int(len(X) * 0.9)\n",
    "\n",
    "# split_index = 4069786\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val = X[:split_index], X[split_index:]\n",
    "y_train, y_val = y[:split_index], y[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d35d37-6c6c-4a62-93cf-9b5b42091ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, LayerNormalization, Embedding\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "\n",
    "# Define some constants for your model (these may need to be adjusted for your specific case)\n",
    "n_features = X.shape[1] # Number of features in the input data\n",
    "embedding_dim = 64 # Dimension of the embedding space\n",
    "num_heads = 4 # Number of heads in the multi-head attention mechanism\n",
    "dff = 256 # Dimension of the feed-forward network\n",
    "num_layers = 4 # Number of Transformer encoder layers\n",
    "\n",
    "# Positional encoding for adding information about sequence order\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "                                     i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "                                     d_model=d_model)\n",
    "        # Apply sin to even indices in the array; cos to odd indices\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "\n",
    "# Transformer encoder layer\n",
    "def transformer_encoder(inputs, embedding_dim, num_heads, dff, rate=0.1):\n",
    "    # Multi-head attention\n",
    "    attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)(inputs, inputs)\n",
    "    # Skip connection and layer normalization\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(inputs + attention_output)\n",
    "    # Feed forward network\n",
    "    ffn_output = Dense(dff, activation='relu')(out1)\n",
    "    ffn_output = Dense(embedding_dim)(ffn_output)\n",
    "    # Skip connection and layer normalization\n",
    "    out2 = LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "    return out2\n",
    "\n",
    "# Input layer\n",
    "inputs = tf.keras.Input(shape=(None, n_features))\n",
    "# Embedding and positional encoding\n",
    "x = Dense(embedding_dim)(inputs)\n",
    "x = PositionalEncoding(n_features, embedding_dim)(x)\n",
    "\n",
    "# Transformer blocks\n",
    "for _ in range(num_layers):\n",
    "    x = transformer_encoder(x, embedding_dim, num_heads, dff)\n",
    "\n",
    "# Output layer\n",
    "x = Dense(50, activation='relu')(x) # Additional dense layer (optional)\n",
    "x = Dense(1, activation='linear')(x) # Output layer\n",
    "\n",
    "# Create the model\n",
    "transformer_model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "# Model summary\n",
    "transformer_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4c4d3c-5d74-47dd-80db-fab3e45ae20c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeccf15-67cf-476d-9033-fdc9c60d1967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "transformer_model.compile(optimizer=Adam(learning_rate=0.001, amsgrad=True), loss='mean_squared_error', \n",
    "                          metrics=['mean_squared_error'])\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = transformer_model.fit(X_train, y_train,\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                validation_data=(X_val, y_val),\n",
    "                                callbacks=[early_stopping])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
